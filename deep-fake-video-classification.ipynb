{"metadata":{"colab":{"provenance":[{"file_id":"1quugzLg7tgAp00MnFwYCiBuflRQTStY8","timestamp":1704606787051}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7620149,"sourceType":"datasetVersion","datasetId":4438552}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/noelwilliam/deep-fake-video-classification?scriptVersionId=162968422\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Importing necessary libraries","metadata":{"id":"ObPER961pFvZ"}},{"cell_type":"code","source":"pip install mtcnn","metadata":{"id":"iYF0B2mszBA4","executionInfo":{"status":"ok","timestamp":1707812158565,"user_tz":-330,"elapsed":9760,"user":{"displayName":"Noel William","userId":"06404726489186135363"}},"outputId":"3497e795-36f6-49f7-d1f6-d154e302dd5f","execution":{"iopub.status.busy":"2024-02-15T12:44:59.562112Z","iopub.execute_input":"2024-02-15T12:44:59.562519Z","iopub.status.idle":"2024-02-15T12:45:13.008196Z","shell.execute_reply.started":"2024-02-15T12:44:59.562465Z","shell.execute_reply":"2024-02-15T12:45:13.006971Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting mtcnn\n  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: keras>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from mtcnn) (2.12.0)\nRequirement already satisfied: opencv-python>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from mtcnn) (4.8.0.76)\nRequirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from opencv-python>=4.1.0->mtcnn) (1.23.5)\nInstalling collected packages: mtcnn\nSuccessfully installed mtcnn-0.1.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport pathlib\nimport cv2\nfrom concurrent.futures import ProcessPoolExecutor\nfrom concurrent.futures import ThreadPoolExecutor\nfrom mtcnn import MTCNN\nimport os\nimport shutil\nimport time\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"id":"706c558d-7604-46d8-a734-755792fcb4c7","executionInfo":{"status":"ok","timestamp":1707812163343,"user_tz":-330,"elapsed":4794,"user":{"displayName":"Noel William","userId":"06404726489186135363"}},"execution":{"iopub.status.busy":"2024-02-15T12:45:13.010712Z","iopub.execute_input":"2024-02-15T12:45:13.011132Z","iopub.status.idle":"2024-02-15T12:45:24.01622Z","shell.execute_reply.started":"2024-02-15T12:45:13.011088Z","shell.execute_reply":"2024-02-15T12:45:24.015444Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Downloading and extraction our training data","metadata":{"id":"zK8KVmNponJJ"}},{"cell_type":"code","source":"!wget https://zenodo.org/record/4068245/files/DeepfakeTIMIT.tar.gz?download=1","metadata":{"executionInfo":{"elapsed":11677,"status":"ok","timestamp":1707812223479,"user":{"displayName":"Noel William","userId":"06404726489186135363"},"user_tz":-330},"id":"56d2b985-53e6-42d4-883a-c92ad1880c08","outputId":"462176af-735b-4ca2-d24d-6e418db24805"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -xf ./DeepfakeTIMIT.tar.gz?download=1","metadata":{"id":"0f243ec3-db50-42bd-bc24-1710f85d0819","executionInfo":{"status":"ok","timestamp":1707812224210,"user_tz":-330,"elapsed":734,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://lp-prod-resources.s3.amazonaws.com/other/detectingdeepfakes/VidTIMIT.zip","metadata":{"executionInfo":{"elapsed":35783,"status":"ok","timestamp":1707812259990,"user":{"displayName":"Noel William","userId":"06404726489186135363"},"user_tz":-330},"id":"d32b632c-6f12-4f82-9958-21c8876be37d","outputId":"bb4c6cb5-76c6-4863-b8b0-b3bc8dfc09a7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip -qq ./VidTIMIT.zip","metadata":{"id":"7390145c-9bc0-4c2b-8b16-e11bbbe7accd","executionInfo":{"status":"ok","timestamp":1707812280437,"user_tz":-330,"elapsed":20452,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf ./__MACOSX","metadata":{"id":"40fdc9d0-a9ef-484d-8abd-24b9a356ef42","executionInfo":{"status":"ok","timestamp":1707812280437,"user_tz":-330,"elapsed":20,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##Making necessary directories","metadata":{"id":"JfBCAYpko29x"}},{"cell_type":"code","source":"!mkdir -p ./Training\\ Videos/preproc_real_vids\n!mkdir -p ./Training\\ Videos/preproc_fake_vids","metadata":{"id":"dfd6137d-b7d4-45bd-8c87-b1b8263e7cd1","executionInfo":{"status":"ok","timestamp":1707812280438,"user_tz":-330,"elapsed":19,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p ./Testing\\ Videos/preproc_real_vids\n!mkdir -p ./Testing\\ Videos/preproc_fake_vids","metadata":{"id":"ac1fbd78-1027-4d21-935e-de0c1bf59952","executionInfo":{"status":"ok","timestamp":1707812281121,"user_tz":-330,"elapsed":700,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir ./Remaining\\ Real\\ Videos","metadata":{"id":"2746e59a-57d6-46f6-b517-769a0ea4abf8","executionInfo":{"status":"ok","timestamp":1707812281122,"user_tz":-330,"elapsed":9,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_fake_vids_paths = [\"./DeepfakeTIMIT/higher_quality\",\"./VidTIMIT\"]","metadata":{"id":"866e8f4c-d664-4dda-9235-980494532a22","executionInfo":{"status":"ok","timestamp":1707812281122,"user_tz":-330,"elapsed":8,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analyzing Training data\nMaking fake and real videos equal in quantity.","metadata":{"id":"TysNg7j0pgti"}},{"cell_type":"code","source":"for base_vid_path in real_fake_vids_paths:\n\n    vid_counter = 0\n    for single_vid_path in pathlib.Path(base_vid_path).glob(\"*/*.avi\"):\n\n        vid_counter += 1\n\n    print(\"Total Number of Videos are {}\".format(vid_counter))\n    vid_counter = 0","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1707812281122,"user":{"displayName":"Noel William","userId":"06404726489186135363"},"user_tz":-330},"id":"355f0f44-ab11-4c87-b00a-a9a12c7ea273","outputId":"b39abe17-b8e9-4f27-8d84-e6535f0ca7a1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vid_counter = 0\n\nfor single_vid_path in pathlib.Path(real_fake_vids_paths[1]).glob(\"*/*.avi\"):\n\n    if str(single_vid_path.parts[-2]) in os.listdir(real_fake_vids_paths[0]):\n        vid_counter += 1\n\n    else:\n        if not os.path.isdir(\"./Remaining Real Videos/\"+single_vid_path.parts[-2]):\n            os.mkdir(\"./Remaining Real Videos/\"+single_vid_path.parts[-2])\n\n        shutil.move(src=single_vid_path,\n                    dst=\"./Remaining Real Videos/\"+\"/\".join([single_vid_path.parts[-2],\n                                                           single_vid_path.parts[-1]]))\n\nprint(\"Total Number of Videos are {}\".format(vid_counter))","metadata":{"executionInfo":{"elapsed":485,"status":"ok","timestamp":1707812325883,"user":{"displayName":"Noel William","userId":"06404726489186135363"},"user_tz":-330},"id":"a1d55d25-69fb-4f0e-8b0f-d498c6ae8cb7","outputId":"eda22de7-5962-47ba-a8e9-67a56a62acef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for base_vid_path in real_fake_vids_paths:\n\n    vid_counter = 0\n    for single_vid_path in pathlib.Path(base_vid_path).glob(\"*/*.avi\"):\n\n        vid_counter += 1\n\n    print(\"Total Number of Videos are {}\".format(vid_counter))","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707812326356,"user":{"displayName":"Noel William","userId":"06404726489186135363"},"user_tz":-330},"id":"905103c7-c96d-440d-9c33-2a7c718cd507","outputId":"f85d430b-d33d-435e-dc21-b574c42325a9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions to preprocess training videos","metadata":{"id":"Xa81P9dspv12"}},{"cell_type":"code","source":"def detect_face(frame):\n\n    face_detector = MTCNN()\n    face_attr = face_detector.detect_faces(frame)\n\n    if len(face_attr) > 0:\n        if len(face_attr[0]['box']) > 0:\n            return face_attr[0]['box'], face_attr[0]['keypoints']['left_eye'], face_attr[0]['keypoints']['right_eye']\n    else:\n        return [None]*3","metadata":{"id":"9c3b1731-9510-43d4-b034-382d9d1b27b7","executionInfo":{"status":"ok","timestamp":1707812329366,"user_tz":-330,"elapsed":4,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def determine_rotation_angle(left_eye_kp, right_eye_kp):\n\n    delta_y = right_eye_kp[1] - left_eye_kp[1]\n    delta_x = right_eye_kp[0] - left_eye_kp[0]\n\n    return np.arctan(delta_y/delta_x)","metadata":{"id":"25894688-5a7f-4100-8efa-02e64dd58217","executionInfo":{"status":"ok","timestamp":1707812329367,"user_tz":-330,"elapsed":4,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def determine_scaling_factor(rescaled_img_size,left_eye_kp,right_eye_kp):\n\n    left_eye_kp = np.array(left_eye_kp)\n    right_eye_kp = np.array(right_eye_kp)\n\n    whole_frame_eye_dist = np.linalg.norm(x=(right_eye_kp - left_eye_kp),ord=2)\n\n    cropped_left_eye_kp = np.array([0.47*rescaled_img_size[0], 0.32*rescaled_img_size[1]])\n    cropped_right_eye_kp = np.array([0.47*rescaled_img_size[0], 0.68*rescaled_img_size[1]])\n\n    cropped_frame_eye_dist = np.linalg.norm(x=(cropped_right_eye_kp - cropped_left_eye_kp),\n                                            ord=2)\n\n    scaling_factor = cropped_frame_eye_dist/whole_frame_eye_dist\n\n    return scaling_factor, cropped_left_eye_kp, cropped_right_eye_kp","metadata":{"id":"eeb93385-0290-45e4-88b2-067336de51e6","executionInfo":{"status":"ok","timestamp":1707812329832,"user_tz":-330,"elapsed":3,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def determine_rotation_center(left_eye_kp, right_eye_kp):\n\n    center_x = (left_eye_kp[0] + right_eye_kp[0])/2\n    center_y = (left_eye_kp[1] + right_eye_kp[1])/2\n\n    return (center_x, center_y)","metadata":{"id":"6da5b795-4a18-414f-92c1-7e5b6cec2eed","executionInfo":{"status":"ok","timestamp":1707812330794,"user_tz":-330,"elapsed":2,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optional Step\nTesting Our functions","metadata":{"id":"P3CV7C7wnmmj"}},{"cell_type":"code","source":"'''\nsingle_vid_path = pathlib.Path(\"./DeepfakeTIMIT/higher_quality/mrgg0/si569-video-msjs1.avi\")\n\nprint(\"\\n\\nGoing to preprocess Video at {}\".format(single_vid_path))\n\nvideo = cv2.VideoCapture(str(single_vid_path))\ncropped_aligned_faces_list = list()\n\nwhile True:\n\n    is_frame, frame = video.read()\n\n    if not is_frame:\n        break\n\n    print(\"Going to preprocess Frame of {} having size {}\".format(single_vid_path.parts[-1],\n                                                                  frame.shape))\n\n    face_bbox, left_eye_kp, right_eye_kp = detect_face(frame)\n\n    if face_bbox == None:\n        continue\n\n    rotation_angle = determine_rotation_angle(left_eye_kp, right_eye_kp);\n    scaling_factor, cropped_left_eye_kp, cropped_right_eye_kp = determine_scaling_factor((224,224),\n                                                                                            left_eye_kp, right_eye_kp)\n    rotation_center = determine_rotation_center(left_eye_kp, right_eye_kp)\n\n    #print(\"Computed everything to get the Rotation Matrix for the Frame\")\n\n    rotation_matrix = cv2.getRotationMatrix2D(center=rotation_center,\n                                                         angle=rotation_angle,scale=scaling_factor)\n\n    #print(\"Computed the Rotation Matrix for the Frame\")\n\n    cropped_center_x = cropped_left_eye_kp[0]\n    cropped_center_y = cropped_left_eye_kp[1] + (np.linalg.norm(x=cropped_right_eye_kp-cropped_right_eye_kp,ord=2)/2)\n    cropped_center = (cropped_center_x, cropped_center_y)\n\n    rotation_matrix[0,2] = rotation_matrix[0,2] - (rotation_center[0] - cropped_center[0])\n    rotation_matrix[1,2] = rotation_matrix[1,2] - (rotation_center[1] - cropped_center[1])\n\n    #print(\"Translated the center in Rotation Matrix to the Cropped Frame Center\")\n\n    cropped_aligned_face = cv2.warpAffine(frame,rotation_matrix,(224,224),\n                                                      cv2.INTER_CUBIC)\n\n    #print(\"Cropped and Aligned the face in the Frame\")\n\n    cropped_aligned_faces_list.append(cropped_aligned_face)\n\n    print(\"Preprocessed Frame of {}\".format(single_vid_path.parts[-1]))\n\n    plt.imshow(cropped_aligned_face)\n\n    break;\n\nvideo.release()\n\ncropped_aligned_faces_list = np.array(cropped_aligned_faces_list)\n\n\nif \"DeepfakeTIMIT\" in list(single_vid_path.parts):\n    np.savez(\"./Training Videos/preproc_fake_vids/{}.npz\".format(str(single_vid_path.parts[-1]).replace(\".avi\",\"\")),\n                             args=cropped_aligned_faces_list)\nelse:\n    np.savez(\"./Training Videos/preproc_real_vids/{}.npz\".format(str(single_vid_path.parts[-1]).replace(\".avi\",\"\")),\n                             args=cropped_aligned_faces_list)\n\nprint(\"\\n\\nProcessed Video at {}\".format(single_vid_path))\n'''","metadata":{"id":"d3be37a7-333b-49ff-948a-e280f96bf29b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ndef preproc_single_vid(single_vid_path):\n\n    print(\"\\n\\nGoing to preprocess Video at {}\\n\".format(single_vid_path))\n\n    video = cv2.VideoCapture(str(single_vid_path))\n    cropped_aligned_faces_list = list()\n\n    while True:\n\n        is_frame, frame = video.read()\n\n        if not is_frame:\n            break\n\n        print(\"Going to preprocess frame of {}\".format(single_vid_path.parts[-1]))\n\n        face_bbox, left_eye_kp, right_eye_kp = detect_face(frame)\n        angle_bw_eyes = determine_rotation_angle(left_eye_kp, right_eye_kp);\n        scaling_factor, cropped_left_eye_kp, cropped_right_eye_kp = determine_scaling_factor((224,224),\n                                                                                        left_eye_kp, right_eye_kp)\n        rotation_center = determine_rotation_center(left_eye_kp, right_eye_kp)\n\n        rotation_matrix = cv2.getRotationMatrix2D(center=rotation_center,\n                                                     angle=rotation_angle,scale=scaling_factor)\n\n        cropped_center_x = cropped_left_eye_kp[0]\n        cropped_center_y = cropped_left_eye_kp[1] + (np.linalg.norm(x=cropped_right_eye_kp-cropped_right_eye_kp,ord=2)/2)\n        cropped_center = (cropped_center_x, cropped_center_y)\n\n        rotation_matrix[0,2] = rotation_matrix[0,2] - (rotation_center[0] - cropped_center[0])\n        rotation_matrix[1,2] = rotation_matrix[1,2] - (rotation_center[1] - cropped_center[1])\n\n        cropped_aligned_face = cv2.warpAffine(frame,rotation_matrix,(224,224),\n                                                  cv2.INTER_CUBIC)\n\n        cropped_aligned_faces_list.append(cropped_aligned_face)\n\n    video.release()\n\n    cropped_aligned_faces_list = np.array(cropped_aligned_faces_list)\n\n    if \"DeepfakeTIMIT\" in list(single_vid_path.parts):\n        np.savez(\"./Training Videos/preproc_fake_vids/{}.npz\".format(single_vid_path.parts[-1]),\n                         args=cropped_aligned_faces_list)\n    else:\n        np.savez(\"./Training Videos/preproc_real_vids/{}.npz\".format(single_vid_path.parts[-1]),\n                         args=cropped_aligned_faces_list)\n\n    print(\"\\n\\nProcessed Video at {}\".format(single_vid_path))\n\"\"\"","metadata":{"id":"b87504ba-6a71-43fe-89b9-d95d6dc06c0b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Function to preprocess a single video\nThis function uses all the function made previously.","metadata":{"id":"pRR9VF0bnzqZ"}},{"cell_type":"code","source":"def preproc_single_vid(mb_vids_paths):\n\n    for single_vid_path in mb_vids_paths:\n\n        print(\"\\n\\nGoing to preprocess Video at {}\".format(single_vid_path))\n\n        video = cv2.VideoCapture(str(single_vid_path))\n        cropped_aligned_faces_list = list()\n\n        while True:\n\n            is_frame, frame = video.read()\n\n            if not is_frame:\n                break\n            \"\"\"\n            print(\"Going to preprocess Frame of {} having size {}\".format(single_vid_path.parts[-1],\n                                                                  frame.shape))\n            \"\"\"\n            face_bbox, left_eye_kp, right_eye_kp = detect_face(frame)\n\n            if face_bbox == None:\n                continue\n\n            rotation_angle = determine_rotation_angle(left_eye_kp, right_eye_kp);\n            scaling_factor, cropped_left_eye_kp, cropped_right_eye_kp = determine_scaling_factor((224,224),\n                                                                                            left_eye_kp, right_eye_kp)\n            rotation_center = determine_rotation_center(left_eye_kp, right_eye_kp)\n\n            #print(\"Computed everything to get the Rotation Matrix for the Frame\")\n\n            rotation_matrix = cv2.getRotationMatrix2D(center=rotation_center,\n                                                         angle=rotation_angle,scale=scaling_factor)\n\n            #print(\"Computed the Rotation Matrix for the Frame\")\n\n            cropped_center_x = cropped_left_eye_kp[0]\n            cropped_center_y = cropped_left_eye_kp[1] + (np.linalg.norm(x=cropped_right_eye_kp-cropped_right_eye_kp,ord=2)/2)\n            cropped_center = (cropped_center_x, cropped_center_y)\n\n            rotation_matrix[0,2] = rotation_matrix[0,2] - (rotation_center[0] - cropped_center[0])\n            rotation_matrix[1,2] = rotation_matrix[1,2] - (rotation_center[1] - cropped_center[1])\n\n            #print(\"Translated the center in Rotation Matrix to the Cropped Frame Center\")\n\n            cropped_aligned_face = cv2.warpAffine(frame,rotation_matrix,(224,224),\n                                                      cv2.INTER_CUBIC)\n\n            #print(\"Cropped and Aligned the face in the Frame\")\n\n            cropped_aligned_faces_list.append(cropped_aligned_face)\n\n            print(\"Preprocessed Frame of {}\".format(single_vid_path.parts[-1]))\n\n        video.release()\n\n        cropped_aligned_faces_list = np.array(cropped_aligned_faces_list)\n\n        if \"DeepfakeTIMIT\" in list(single_vid_path.parts):\n            np.savez(\"./Training Videos/preproc_fake_vids/{}.npz\".format(str(single_vid_path.parts[-1]).replace(\".avi\",\"\")),\n                             args=cropped_aligned_faces_list)\n        else:\n            np.savez(\"./Training Videos/preproc_real_vids/{}.npz\".format(str(single_vid_path.parts[-1]).replace(\".avi\",\"\")),\n                             args=cropped_aligned_faces_list)\n\n        print(\"\\n\\nProcessed Video at {}\".format(single_vid_path))","metadata":{"id":"9ab3b951-5498-4e7e-a3f6-93ab42bcfd70","executionInfo":{"status":"ok","timestamp":1707812339706,"user_tz":-330,"elapsed":563,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing all videos","metadata":{"id":"TemjW9VzoAwE"}},{"cell_type":"code","source":"from concurrent.futures import ThreadPoolExecutor\nfrom collections import deque\n\nvid_counter = 0\nmb_number = 0\n\nreal_fake_preproc_vids_paths = [\"./Training Videos/preproc_fake_vids\",\n                               \"./Training Videos/preproc_real_vids\"]\n\nfor vids_path, preproc_vids_path in zip(real_fake_vids_paths,\n                                                    real_fake_preproc_vids_paths):\n\n    vids2preproc_path_list = deque()\n\n    for single_vid_path in pathlib.Path(vids_path).glob(\"*/*.avi\"):\n\n        vids2preproc_path_list.append(single_vid_path)\n        vid_counter += 1\n\n        if vid_counter % (os.cpu_count()) == 0:\n\n            start_time = time.time()\n\n            with ThreadPoolExecutor(max_workers=(os.cpu_count())) as pool:\n                future = pool.submit(preproc_single_vid, list(vids2preproc_path_list))\n                future.result()\n\n            end_time = time.time()\n            mb_number += 1\n            elapsed_time = end_time - start_time\n\n            print(\"\\n\\nProcessed Mini Batch # {} of 64 Videos in {} seconds\".format(mb_number,elapsed_time))\n\n            vids2preproc_path_list.clear()\n","metadata":{"id":"0h_q1c4YqynY","outputId":"44da84a6-f505-4241-acc2-84a36c0d4ecc","scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Storing Training Videos\nSeperately storing training fake and real videos in seperate list or numpy arrays","metadata":{"id":"47N6UP6-lSTo"}},{"cell_type":"code","source":"training_real_vids_paths = list()\ntraining_fake_vids_paths = list()\n\nfor single_img_path in pathlib.Path(\"/kaggle/input/deep-fake-and-real-videos/Training Videos\").glob(\"*/*.npz\"):\n\n    if \"real\" in str(single_img_path.parts[-2]).split(\"_\"):\n        training_real_vids_paths.append(str(single_img_path))\n\n    elif \"fake\" in str(single_img_path.parts[-2]).split(\"_\"):\n        training_fake_vids_paths.append(str(single_img_path))\n\ntraining_real_vids_paths = np.array(training_real_vids_paths)\ntraining_fake_vids_paths = np.array(training_fake_vids_paths)","metadata":{"id":"ed592b1d-e922-4600-a488-043c7a17e9dc","execution":{"iopub.status.busy":"2024-02-15T12:45:24.017266Z","iopub.execute_input":"2024-02-15T12:45:24.017756Z","iopub.status.idle":"2024-02-15T12:45:24.281686Z","shell.execute_reply.started":"2024-02-15T12:45:24.017731Z","shell.execute_reply":"2024-02-15T12:45:24.280767Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"len(training_real_vids_paths)","metadata":{"id":"s7yBsj4yQxLH","executionInfo":{"status":"ok","timestamp":1707757216235,"user_tz":-330,"elapsed":4,"user":{"displayName":"Noel William","userId":"06404726489186135363"}},"outputId":"621b4324-04cf-46f9-b6ce-4590e24467fc","execution":{"iopub.status.busy":"2024-02-15T12:45:24.283753Z","iopub.execute_input":"2024-02-15T12:45:24.284027Z","iopub.status.idle":"2024-02-15T12:45:24.291045Z","shell.execute_reply.started":"2024-02-15T12:45:24.284002Z","shell.execute_reply":"2024-02-15T12:45:24.290065Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"197"},"metadata":{}}]},{"cell_type":"code","source":"len(training_fake_vids_paths)","metadata":{"id":"aSKWUuExQyr2","executionInfo":{"status":"ok","timestamp":1707757241901,"user_tz":-330,"elapsed":756,"user":{"displayName":"Noel William","userId":"06404726489186135363"}},"outputId":"1190534a-206e-4438-90a6-02ee798fbce7","execution":{"iopub.status.busy":"2024-02-15T12:45:24.292178Z","iopub.execute_input":"2024-02-15T12:45:24.292454Z","iopub.status.idle":"2024-02-15T12:45:24.301901Z","shell.execute_reply.started":"2024-02-15T12:45:24.292432Z","shell.execute_reply":"2024-02-15T12:45:24.301134Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"320"},"metadata":{}}]},{"cell_type":"code","source":"# Calculating how many times to repeat the real videos\nrepeat_times = len(training_fake_vids_paths) // len(training_real_vids_paths)\n\n# Repeat the real videos\noversampled_real_vids_paths = np.repeat(training_real_vids_paths, repeat_times)\n\n# If there are still fewer real videos than fake videos, append some real videos to make up the difference\nremainder = len(training_fake_vids_paths) - len(oversampled_real_vids_paths)\nif remainder > 0:\n    oversampled_real_vids_paths = np.concatenate((oversampled_real_vids_paths, training_real_vids_paths[:remainder]))\n\n# Now oversampled_real_vids_paths should have the same length as training_fake_vids_paths\nassert len(oversampled_real_vids_paths) == len(training_fake_vids_paths)","metadata":{"id":"696pixNASaye","execution":{"iopub.status.busy":"2024-02-15T12:45:24.30294Z","iopub.execute_input":"2024-02-15T12:45:24.303216Z","iopub.status.idle":"2024-02-15T12:45:24.312015Z","shell.execute_reply.started":"2024-02-15T12:45:24.303185Z","shell.execute_reply":"2024-02-15T12:45:24.311303Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"training_real_vids_paths = oversampled_real_vids_paths\nlen(training_real_vids_paths)","metadata":{"id":"KTBYvXIcSf7m","executionInfo":{"status":"ok","timestamp":1707757775177,"user_tz":-330,"elapsed":4,"user":{"displayName":"Noel William","userId":"06404726489186135363"}},"outputId":"783edede-336a-4995-a731-0e3f8c93d7ef","execution":{"iopub.status.busy":"2024-02-15T12:45:24.312987Z","iopub.execute_input":"2024-02-15T12:45:24.313247Z","iopub.status.idle":"2024-02-15T12:45:24.323554Z","shell.execute_reply.started":"2024-02-15T12:45:24.313215Z","shell.execute_reply":"2024-02-15T12:45:24.322613Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"320"},"metadata":{}}]},{"cell_type":"markdown","source":"#Training Data Generator","metadata":{"id":"4GvSXC0omClt"}},{"cell_type":"code","source":"''' \ndef custom_training_data_generator(mb_size, target_frame_size=(224, 224), max_frames=100):\n    for i in range(len(training_fake_vids_paths) // 2):\n        X_train_mb = []\n        Y_train_mb = []\n\n        training_fake_mb_vids_paths = training_fake_vids_paths[i * (mb_size // 2):(i + 1) * (mb_size // 2)]\n        training_real_mb_vids_paths = training_real_vids_paths[i * (mb_size // 2):(i + 1) * (mb_size // 2)]\n\n        for single_fake_vid_path, single_real_vid_path in zip(training_fake_mb_vids_paths,\n                                                              training_real_mb_vids_paths):\n\n            fake_frames = np.load(file=single_fake_vid_path)['args']\n            # Setting image to (224, 224) if not already\n            fake_frames_resized = [cv2.resize(frame, target_frame_size) for frame in fake_frames]\n            fake_frames_resized += [np.zeros_like(fake_frames_resized[0])] * (max_frames - len(fake_frames_resized))\n            fake_frames_resized = fake_frames_resized[:max_frames]  # This ensures 100 frames\n\n\n            real_frames = np.load(file=single_real_vid_path)['args']\n\n            real_frames_resized = [cv2.resize(frame, target_frame_size) for frame in real_frames]\n            real_frames_resized += [np.zeros_like(real_frames_resized[0])] * (max_frames - len(real_frames_resized))\n            real_frames_resized = real_frames_resized[:max_frames]\n\n            X_train_mb.append(real_frames_resized)\n            X_train_mb.append(fake_frames_resized)\n\n            # Real labeled as 0 and Fake labeled as 1\n            Y_train_mb.append(0)\n            Y_train_mb.append(1)\n\n        X_train_mb = np.array(X_train_mb)\n        Y_train_mb = np.array(Y_train_mb)\n\n        yield X_train_mb, Y_train_mb\n    '''","metadata":{"id":"67kD8wBsvbF2","execution":{"iopub.status.busy":"2024-02-15T12:45:24.324573Z","iopub.execute_input":"2024-02-15T12:45:24.324798Z","iopub.status.idle":"2024-02-15T12:45:24.336228Z","shell.execute_reply.started":"2024-02-15T12:45:24.324778Z","shell.execute_reply":"2024-02-15T12:45:24.335325Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"\" \\ndef custom_training_data_generator(mb_size, target_frame_size=(224, 224), max_frames=100):\\n    for i in range(len(training_fake_vids_paths) // 2):\\n        X_train_mb = []\\n        Y_train_mb = []\\n\\n        training_fake_mb_vids_paths = training_fake_vids_paths[i * (mb_size // 2):(i + 1) * (mb_size // 2)]\\n        training_real_mb_vids_paths = training_real_vids_paths[i * (mb_size // 2):(i + 1) * (mb_size // 2)]\\n\\n        for single_fake_vid_path, single_real_vid_path in zip(training_fake_mb_vids_paths,\\n                                                              training_real_mb_vids_paths):\\n\\n            fake_frames = np.load(file=single_fake_vid_path)['args']\\n            # Setting image to (224, 224) if not already\\n            fake_frames_resized = [cv2.resize(frame, target_frame_size) for frame in fake_frames]\\n            fake_frames_resized += [np.zeros_like(fake_frames_resized[0])] * (max_frames - len(fake_frames_resized))\\n            fake_frames_resized = fake_frames_resized[:max_frames]  # This ensures 100 frames\\n\\n\\n            real_frames = np.load(file=single_real_vid_path)['args']\\n\\n            real_frames_resized = [cv2.resize(frame, target_frame_size) for frame in real_frames]\\n            real_frames_resized += [np.zeros_like(real_frames_resized[0])] * (max_frames - len(real_frames_resized))\\n            real_frames_resized = real_frames_resized[:max_frames]\\n\\n            X_train_mb.append(real_frames_resized)\\n            X_train_mb.append(fake_frames_resized)\\n\\n            # Real labeled as 0 and Fake labeled as 1\\n            Y_train_mb.append(0)\\n            Y_train_mb.append(1)\\n\\n        X_train_mb = np.array(X_train_mb)\\n        Y_train_mb = np.array(Y_train_mb)\\n\\n        yield X_train_mb, Y_train_mb\\n    \""},"metadata":{}}]},{"cell_type":"code","source":"def custom_training_data_generator(mb_size, target_frame_size=(224, 224), max_frames=100):\n    while True:  #works like repeat()\n        for i in range(len(training_fake_vids_paths) // 2):\n            X_train_mb = []\n            Y_train_mb = []\n\n            training_fake_mb_vids_paths = training_fake_vids_paths[i * (mb_size // 2):(i + 1) * (mb_size // 2)]\n            training_real_mb_vids_paths = training_real_vids_paths[i * (mb_size // 2):(i + 1) * (mb_size // 2)]\n\n            for single_fake_vid_path, single_real_vid_path in zip(training_fake_mb_vids_paths,\n                                                                  training_real_mb_vids_paths):\n\n                fake_frames = np.load(file=single_fake_vid_path)['args']\n                # Setting image to (224, 224) if not already\n                fake_frames_resized = [cv2.resize(frame, target_frame_size) for frame in fake_frames]\n                fake_frames_resized += [np.zeros_like(fake_frames_resized[0])] * (max_frames - len(fake_frames_resized))\n                fake_frames_resized = fake_frames_resized[:max_frames]  # This ensures 100 frames\n\n\n                real_frames = np.load(file=single_real_vid_path)['args']\n\n                real_frames_resized = [cv2.resize(frame, target_frame_size) for frame in real_frames]\n                real_frames_resized += [np.zeros_like(real_frames_resized[0])] * (max_frames - len(real_frames_resized))\n                real_frames_resized = real_frames_resized[:max_frames]\n\n                X_train_mb.append(real_frames_resized)\n                X_train_mb.append(fake_frames_resized)\n\n                # Real labeled as 0 and Fake labeled as 1\n                Y_train_mb.append(0)\n                Y_train_mb.append(1)\n\n            X_train_mb = np.array(X_train_mb)\n            Y_train_mb = np.array(Y_train_mb)\n\n            yield X_train_mb, Y_train_mb","metadata":{"id":"PUGiJg90V_gs","execution":{"iopub.status.busy":"2024-02-15T12:45:24.337336Z","iopub.execute_input":"2024-02-15T12:45:24.337671Z","iopub.status.idle":"2024-02-15T12:45:24.351598Z","shell.execute_reply.started":"2024-02-15T12:45:24.337639Z","shell.execute_reply":"2024-02-15T12:45:24.35081Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Split training data for validation data\nCreating seperate data for validation same as training data","metadata":{"id":"JfmMXW_-mMVu"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Combine real and fake paths\nall_vids_paths = np.concatenate((training_real_vids_paths, training_fake_vids_paths))\n\n# Create labels: 0 for real and 1 for fake\nall_vids_labels = np.concatenate((np.zeros(len(training_real_vids_paths)), np.ones(len(training_fake_vids_paths))))\n\n# Split the data into training and validation sets (80-20 split here)\ntrain_vids_paths, val_vids_paths, train_vids_labels, val_vids_labels = train_test_split(all_vids_paths, all_vids_labels, test_size=0.2, random_state=42)\n\n# Separate real and fake validation video paths\nvalidation_real_vids_paths = val_vids_paths[val_vids_labels == 0]\nvalidation_fake_vids_paths = val_vids_paths[val_vids_labels == 1]\n","metadata":{"id":"FDDyTvLfKWlC","execution":{"iopub.status.busy":"2024-02-15T12:45:24.356107Z","iopub.execute_input":"2024-02-15T12:45:24.356365Z","iopub.status.idle":"2024-02-15T12:45:24.755276Z","shell.execute_reply.started":"2024-02-15T12:45:24.356343Z","shell.execute_reply":"2024-02-15T12:45:24.75428Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"#Validation Data Generator","metadata":{"id":"UXuBpGfimbRQ"}},{"cell_type":"code","source":"      '''\n        def custom_validation_data_generator(mb_size, target_frame_size=(224, 224), max_frames=100):\n    for i in range(len(validation_fake_vids_paths) // 2):\n        X_val_mb = []\n        Y_val_mb = []\n\n        validation_fake_mb_vids_paths = validation_fake_vids_paths[i * (mb_size // 2):(i + 1) * (mb_size // 2)]\n        validation_real_mb_vids_paths = validation_real_vids_paths[i * (mb_size // 2):(i + 1) * (mb_size // 2)]\n\n        for single_fake_vid_path, single_real_vid_path in zip(validation_fake_mb_vids_paths,\n                                                              validation_real_mb_vids_paths):\n\n            fake_frames = np.load(file=single_fake_vid_path)['args']\n            # Setting image to (224, 224) if not already\n            fake_frames_resized = [cv2.resize(frame, target_frame_size) for frame in fake_frames]\n            fake_frames_resized += [np.zeros_like(fake_frames_resized[0])] * (max_frames - len(fake_frames_resized))\n            fake_frames_resized = fake_frames_resized[:max_frames]  # This ensures 100 frames\n\n\n            real_frames = np.load(file=single_real_vid_path)['args']\n\n            real_frames_resized = [cv2.resize(frame, target_frame_size) for frame in real_frames]\n            real_frames_resized += [np.zeros_like(real_frames_resized[0])] * (max_frames - len(real_frames_resized))\n            real_frames_resized = real_frames_resized[:max_frames]\n\n            X_val_mb.append(real_frames_resized)\n            X_val_mb.append(fake_frames_resized)\n\n            # Real labeled as 0 and Fake labeled as 1\n            Y_val_mb.append(0)\n            Y_val_mb.append(1)\n\n        X_val_mb = np.array(X_val_mb)\n        Y_val_mb = np.array(Y_val_mb)\n\n        yield X_val_mb, Y_val_mb\n        \n        '''","metadata":{"id":"VS9q3TIYKTxD","execution":{"iopub.status.busy":"2024-02-15T12:45:24.75659Z","iopub.execute_input":"2024-02-15T12:45:24.75694Z","iopub.status.idle":"2024-02-15T12:45:24.765315Z","shell.execute_reply.started":"2024-02-15T12:45:24.756905Z","shell.execute_reply":"2024-02-15T12:45:24.76428Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"\"\\n  def custom_validation_data_generator(mb_size, target_frame_size=(224, 224), max_frames=100):\\n    for i in range(len(validation_fake_vids_paths) // 2):\\n  X_val_mb = []\\n  Y_val_mb = []\\n\\n  validation_fake_mb_vids_paths = validation_fake_vids_paths[i * (mb_size // 2):(i + 1) * (mb_size // 2)]\\n  validation_real_mb_vids_paths = validation_real_vids_paths[i * (mb_size // 2):(i + 1) * (mb_size // 2)]\\n\\n  for single_fake_vid_path, single_real_vid_path in zip(validation_fake_mb_vids_paths,\\n                                                        validation_real_mb_vids_paths):\\n\\n      fake_frames = np.load(file=single_fake_vid_path)['args']\\n      # Setting image to (224, 224) if not already\\n      fake_frames_resized = [cv2.resize(frame, target_frame_size) for frame in fake_frames]\\n      fake_frames_resized += [np.zeros_like(fake_frames_resized[0])] * (max_frames - len(fake_frames_resized))\\n      fake_frames_resized = fake_frames_resized[:max_frames]  # This ensures 100 frames\\n\\n\\n      real_frames = np.load(file=single_real_vid_path)['args']\\n\\n      real_frames_resized = [cv2.resize(frame, target_frame_size) for frame in real_frames]\\n      real_frames_resized += [np.zeros_like(real_frames_resized[0])] * (max_frames - len(real_frames_resized))\\n      real_frames_resized = real_frames_resized[:max_frames]\\n\\n      X_val_mb.append(real_frames_resized)\\n      X_val_mb.append(fake_frames_resized)\\n\\n      # Real labeled as 0 and Fake labeled as 1\\n      Y_val_mb.append(0)\\n      Y_val_mb.append(1)\\n\\n  X_val_mb = np.array(X_val_mb)\\n  Y_val_mb = np.array(Y_val_mb)\\n\\n  yield X_val_mb, Y_val_mb\\n  \\n  \""},"metadata":{}}]},{"cell_type":"code","source":"def custom_validation_data_generator(mb_size, target_frame_size=(224, 224), max_frames=100):\n    while True:  # works like repeat()\n        for i in range(len(validation_fake_vids_paths) // 2):\n            X_val_mb = []\n            Y_val_mb = []\n\n            validation_fake_mb_vids_paths = validation_fake_vids_paths[i * (mb_size // 2):(i + 1) * (mb_size // 2)]\n            validation_real_mb_vids_paths = validation_real_vids_paths[i * (mb_size // 2):(i + 1) * (mb_size // 2)]\n\n            for single_fake_vid_path, single_real_vid_path in zip(validation_fake_mb_vids_paths,\n                                                                  validation_real_mb_vids_paths):\n\n                fake_frames = np.load(file=single_fake_vid_path)['args']\n                # Setting image to (224, 224) if not already\n                fake_frames_resized = [cv2.resize(frame, target_frame_size) for frame in fake_frames]\n                fake_frames_resized += [np.zeros_like(fake_frames_resized[0])] * (max_frames - len(fake_frames_resized))\n                fake_frames_resized = fake_frames_resized[:max_frames]  # This ensures 100 frames\n\n\n                real_frames = np.load(file=single_real_vid_path)['args']\n\n                real_frames_resized = [cv2.resize(frame, target_frame_size) for frame in real_frames]\n                real_frames_resized += [np.zeros_like(real_frames_resized[0])] * (max_frames - len(real_frames_resized))\n                real_frames_resized = real_frames_resized[:max_frames]\n\n                X_val_mb.append(real_frames_resized)\n                X_val_mb.append(fake_frames_resized)\n\n                # Real labeled as 0 and Fake labeled as 1\n                Y_val_mb.append(0)\n                Y_val_mb.append(1)\n\n            X_val_mb = np.array(X_val_mb)\n            Y_val_mb = np.array(Y_val_mb)\n\n            yield X_val_mb, Y_val_mb","metadata":{"id":"Nt7maX4EWCYk","execution":{"iopub.status.busy":"2024-02-15T12:45:24.766475Z","iopub.execute_input":"2024-02-15T12:45:24.76679Z","iopub.status.idle":"2024-02-15T12:45:24.780608Z","shell.execute_reply.started":"2024-02-15T12:45:24.766766Z","shell.execute_reply":"2024-02-15T12:45:24.779571Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"#Initializing data Generators","metadata":{"id":"mTYuosYSnGHy"}},{"cell_type":"markdown","source":"Initializing our data generator and checking shape of data for our model","metadata":{"id":"cpUqq-sLmir0"}},{"cell_type":"code","source":"train_datagen = custom_training_data_generator(4)","metadata":{"id":"47184945-cc23-45ad-b452-89152cc83ce7","execution":{"iopub.status.busy":"2024-02-15T12:45:24.781752Z","iopub.execute_input":"2024-02-15T12:45:24.782082Z","iopub.status.idle":"2024-02-15T12:45:24.795241Z","shell.execute_reply.started":"2024-02-15T12:45:24.782054Z","shell.execute_reply":"2024-02-15T12:45:24.794549Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"val_datagen = custom_validation_data_generator(4)","metadata":{"id":"284QmVfaLi6W","execution":{"iopub.status.busy":"2024-02-15T12:45:24.796142Z","iopub.execute_input":"2024-02-15T12:45:24.796396Z","iopub.status.idle":"2024-02-15T12:45:24.80653Z","shell.execute_reply.started":"2024-02-15T12:45:24.796372Z","shell.execute_reply":"2024-02-15T12:45:24.804516Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"X_train_mb, Y_train_mb = next(train_datagen)","metadata":{"id":"58ff5239-15b3-4a37-8f64-f966b222a59e","execution":{"iopub.status.busy":"2024-02-15T12:45:24.807753Z","iopub.execute_input":"2024-02-15T12:45:24.808086Z","iopub.status.idle":"2024-02-15T12:45:25.6854Z","shell.execute_reply.started":"2024-02-15T12:45:24.808055Z","shell.execute_reply":"2024-02-15T12:45:25.684485Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"X_train_mb.shape","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1707758755930,"user":{"displayName":"Noel William","userId":"06404726489186135363"},"user_tz":-330},"id":"ca140a8a-ed6c-487a-9c2d-201408c1e512","outputId":"7564ac62-56f0-4c7b-cfe7-d6910a76462e","execution":{"iopub.status.busy":"2024-02-15T12:45:25.686488Z","iopub.execute_input":"2024-02-15T12:45:25.686807Z","iopub.status.idle":"2024-02-15T12:45:25.692825Z","shell.execute_reply.started":"2024-02-15T12:45:25.686781Z","shell.execute_reply":"2024-02-15T12:45:25.691949Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(4, 100, 224, 224, 3)"},"metadata":{}}]},{"cell_type":"code","source":"Y_train_mb.shape","metadata":{"executionInfo":{"elapsed":1130,"status":"ok","timestamp":1707758757971,"user":{"displayName":"Noel William","userId":"06404726489186135363"},"user_tz":-330},"id":"qMO6dF0Vb3il","outputId":"55b28f54-5a96-4b05-d368-61bfe65cfb54","execution":{"iopub.status.busy":"2024-02-15T12:45:25.693969Z","iopub.execute_input":"2024-02-15T12:45:25.69431Z","iopub.status.idle":"2024-02-15T12:45:25.706385Z","shell.execute_reply.started":"2024-02-15T12:45:25.694268Z","shell.execute_reply":"2024-02-15T12:45:25.705551Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(4,)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Neural Network","metadata":{"id":"QMfPNXZYmtx_"}},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import TimeDistributed, LSTM, Dense, Flatten, Dropout, Input\nfrom tensorflow.keras.applications import ResNet50","metadata":{"id":"Z-ymocTZe-Bk","execution":{"iopub.status.busy":"2024-02-15T12:45:25.707309Z","iopub.execute_input":"2024-02-15T12:45:25.707576Z","iopub.status.idle":"2024-02-15T12:45:25.722074Z","shell.execute_reply.started":"2024-02-15T12:45:25.707554Z","shell.execute_reply":"2024-02-15T12:45:25.721055Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def create_model():\n\n    base_model = ResNet50(include_top=False, input_shape=(224, 224, 3))\n    base_model.trainable = False\n\n    inputs = Input(shape=(100, 224, 224, 3))\n\n    x = TimeDistributed(base_model)(inputs)\n\n    x = TimeDistributed(Flatten())(x)\n\n    x = LSTM(50, return_sequences = True)(x)\n    \n    x = LSTM(50, return_sequences = True)(x)\n    \n    x = LSTM(50, return_sequences = False)(x)\n\n    x = Dropout(0.25)(x)\n\n    outputs = Dense(1, activation=\"sigmoid\")(x)\n\n    model = Model(inputs=inputs, outputs=outputs)\n\n    return model","metadata":{"id":"e8573d32-ef0e-4c36-833c-eb1741366147","execution":{"iopub.status.busy":"2024-02-15T12:46:41.375113Z","iopub.execute_input":"2024-02-15T12:46:41.375509Z","iopub.status.idle":"2024-02-15T12:46:41.382537Z","shell.execute_reply.started":"2024-02-15T12:46:41.375477Z","shell.execute_reply":"2024-02-15T12:46:41.381673Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model = create_model()","metadata":{"id":"nXIZTsT6ez3_","execution":{"iopub.status.busy":"2024-02-15T12:46:45.068359Z","iopub.execute_input":"2024-02-15T12:46:45.069116Z","iopub.status.idle":"2024-02-15T12:46:50.431218Z","shell.execute_reply.started":"2024-02-15T12:46:45.069085Z","shell.execute_reply":"2024-02-15T12:46:50.430251Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n94765736/94765736 [==============================] - 0s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"id":"8utQvokZe0p1","execution":{"iopub.status.busy":"2024-02-15T12:46:50.433187Z","iopub.execute_input":"2024-02-15T12:46:50.433828Z","iopub.status.idle":"2024-02-15T12:46:50.45378Z","shell.execute_reply.started":"2024-02-15T12:46:50.433791Z","shell.execute_reply":"2024-02-15T12:46:50.452846Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1707758764574,"user":{"displayName":"Noel William","userId":"06404726489186135363"},"user_tz":-330},"id":"AZVsi50ogfAK","outputId":"2646a788-d616-4728-f2de-5a79ae58d358","execution":{"iopub.status.busy":"2024-02-15T12:46:50.454894Z","iopub.execute_input":"2024-02-15T12:46:50.455155Z","iopub.status.idle":"2024-02-15T12:46:50.496785Z","shell.execute_reply.started":"2024-02-15T12:46:50.455132Z","shell.execute_reply":"2024-02-15T12:46:50.495994Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 100, 224, 224, 3  0         \n                             )]                                  \n                                                                 \n time_distributed (TimeDistr  (None, 100, 7, 7, 2048)  23587712  \n ibuted)                                                         \n                                                                 \n time_distributed_1 (TimeDis  (None, 100, 100352)      0         \n tributed)                                                       \n                                                                 \n lstm (LSTM)                 (None, 100, 50)           20080600  \n                                                                 \n lstm_1 (LSTM)               (None, 100, 50)           20200     \n                                                                 \n lstm_2 (LSTM)               (None, 50)                20200     \n                                                                 \n dropout (Dropout)           (None, 50)                0         \n                                                                 \n dense (Dense)               (None, 1)                 51        \n                                                                 \n=================================================================\nTotal params: 43,708,763\nTrainable params: 20,121,051\nNon-trainable params: 23,587,712\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#Fitting Our Model","metadata":{"id":"JNXTYxJQm8hg"}},{"cell_type":"code","source":"train_steps_per_epoch = len(train_vids_paths) // 4  # mini-batch size is 4\nval_steps = len(val_vids_paths) // 4","metadata":{"id":"-C4eg65WMuT7","execution":{"iopub.status.busy":"2024-02-15T12:46:50.498554Z","iopub.execute_input":"2024-02-15T12:46:50.498816Z","iopub.status.idle":"2024-02-15T12:46:50.502782Z","shell.execute_reply.started":"2024-02-15T12:46:50.498793Z","shell.execute_reply":"2024-02-15T12:46:50.501787Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model.fit(train_datagen, steps_per_epoch=train_steps_per_epoch,\n                    validation_data=val_datagen, validation_steps=val_steps,\n                    epochs=10)","metadata":{"id":"QJn90vQdMwTc","executionInfo":{"status":"error","timestamp":1707760273449,"user_tz":-330,"elapsed":558,"user":{"displayName":"Noel William","userId":"06404726489186135363"}},"outputId":"ef4b0554-ac9d-457e-a3f4-79b5d7710317","execution":{"iopub.status.busy":"2024-02-15T12:46:50.503775Z","iopub.execute_input":"2024-02-15T12:46:50.504074Z","iopub.status.idle":"2024-02-15T13:19:52.132902Z","shell.execute_reply.started":"2024-02-15T12:46:50.504049Z","shell.execute_reply":"2024-02-15T13:19:52.131974Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Epoch 1/10\n128/128 [==============================] - 218s 2s/step - loss: 0.6947 - accuracy: 0.5254 - val_loss: 0.6912 - val_accuracy: 0.5703\nEpoch 2/10\n128/128 [==============================] - 196s 2s/step - loss: 0.6925 - accuracy: 0.5137 - val_loss: 0.6949 - val_accuracy: 0.5000\nEpoch 3/10\n128/128 [==============================] - 196s 2s/step - loss: 0.7005 - accuracy: 0.4824 - val_loss: 0.6931 - val_accuracy: 0.5000\nEpoch 4/10\n128/128 [==============================] - 196s 2s/step - loss: 0.6992 - accuracy: 0.5020 - val_loss: 0.6929 - val_accuracy: 0.5078\nEpoch 5/10\n128/128 [==============================] - 196s 2s/step - loss: 0.6911 - accuracy: 0.5332 - val_loss: 0.6932 - val_accuracy: 0.5000\nEpoch 6/10\n128/128 [==============================] - 196s 2s/step - loss: 0.6944 - accuracy: 0.5078 - val_loss: 0.6930 - val_accuracy: 0.5000\nEpoch 7/10\n128/128 [==============================] - 196s 2s/step - loss: 0.6981 - accuracy: 0.4902 - val_loss: 0.6934 - val_accuracy: 0.5000\nEpoch 8/10\n128/128 [==============================] - 196s 2s/step - loss: 0.6914 - accuracy: 0.4961 - val_loss: 0.6931 - val_accuracy: 0.5000\nEpoch 9/10\n128/128 [==============================] - 196s 2s/step - loss: 0.6930 - accuracy: 0.5293 - val_loss: 0.6927 - val_accuracy: 0.5078\nEpoch 10/10\n128/128 [==============================] - 196s 2s/step - loss: 0.6992 - accuracy: 0.5059 - val_loss: 0.6931 - val_accuracy: 0.5000\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x79fbc1bcf550>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"JkEw_frSQf7u"},"execution_count":null,"outputs":[]}]}