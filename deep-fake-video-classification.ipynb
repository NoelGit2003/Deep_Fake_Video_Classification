{"metadata":{"colab":{"provenance":[{"file_id":"1quugzLg7tgAp00MnFwYCiBuflRQTStY8","timestamp":1704606787051}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7620149,"sourceType":"datasetVersion","datasetId":4438552}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/noelwilliam/deep-fake-video-classification?scriptVersionId=162787498\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Importing necessary libraries","metadata":{"id":"ObPER961pFvZ"}},{"cell_type":"code","source":"pip install mtcnn","metadata":{"id":"iYF0B2mszBA4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707812158565,"user_tz":-330,"elapsed":9760,"user":{"displayName":"Noel William","userId":"06404726489186135363"}},"outputId":"3497e795-36f6-49f7-d1f6-d154e302dd5f","execution":{"iopub.status.busy":"2024-02-13T21:45:59.274337Z","iopub.execute_input":"2024-02-13T21:45:59.274629Z","iopub.status.idle":"2024-02-13T21:46:12.501947Z","shell.execute_reply.started":"2024-02-13T21:45:59.274605Z","shell.execute_reply":"2024-02-13T21:46:12.500708Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting mtcnn\n  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: keras>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from mtcnn) (2.12.0)\nRequirement already satisfied: opencv-python>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from mtcnn) (4.8.0.76)\nRequirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from opencv-python>=4.1.0->mtcnn) (1.23.5)\nInstalling collected packages: mtcnn\nSuccessfully installed mtcnn-0.1.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport pathlib\nimport cv2\nfrom concurrent.futures import ProcessPoolExecutor\nfrom concurrent.futures import ThreadPoolExecutor\nfrom mtcnn import MTCNN\nimport os\nimport shutil\nimport time\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"id":"706c558d-7604-46d8-a734-755792fcb4c7","executionInfo":{"status":"ok","timestamp":1707812163343,"user_tz":-330,"elapsed":4794,"user":{"displayName":"Noel William","userId":"06404726489186135363"}},"execution":{"iopub.status.busy":"2024-02-13T21:46:12.50543Z","iopub.execute_input":"2024-02-13T21:46:12.505779Z","iopub.status.idle":"2024-02-13T21:46:12.520436Z","shell.execute_reply.started":"2024-02-13T21:46:12.50575Z","shell.execute_reply":"2024-02-13T21:46:12.519447Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Downloading and extraction our training data","metadata":{"id":"zK8KVmNponJJ"}},{"cell_type":"code","source":"!wget https://zenodo.org/record/4068245/files/DeepfakeTIMIT.tar.gz?download=1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11677,"status":"ok","timestamp":1707812223479,"user":{"displayName":"Noel William","userId":"06404726489186135363"},"user_tz":-330},"id":"56d2b985-53e6-42d4-883a-c92ad1880c08","outputId":"462176af-735b-4ca2-d24d-6e418db24805"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -xf ./DeepfakeTIMIT.tar.gz?download=1","metadata":{"id":"0f243ec3-db50-42bd-bc24-1710f85d0819","executionInfo":{"status":"ok","timestamp":1707812224210,"user_tz":-330,"elapsed":734,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://lp-prod-resources.s3.amazonaws.com/other/detectingdeepfakes/VidTIMIT.zip","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35783,"status":"ok","timestamp":1707812259990,"user":{"displayName":"Noel William","userId":"06404726489186135363"},"user_tz":-330},"id":"d32b632c-6f12-4f82-9958-21c8876be37d","outputId":"bb4c6cb5-76c6-4863-b8b0-b3bc8dfc09a7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip -qq ./VidTIMIT.zip","metadata":{"id":"7390145c-9bc0-4c2b-8b16-e11bbbe7accd","executionInfo":{"status":"ok","timestamp":1707812280437,"user_tz":-330,"elapsed":20452,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf ./__MACOSX","metadata":{"id":"40fdc9d0-a9ef-484d-8abd-24b9a356ef42","executionInfo":{"status":"ok","timestamp":1707812280437,"user_tz":-330,"elapsed":20,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##Making necessary directories","metadata":{"id":"JfBCAYpko29x"}},{"cell_type":"code","source":"!mkdir -p ./Training\\ Videos/preproc_real_vids\n!mkdir -p ./Training\\ Videos/preproc_fake_vids","metadata":{"id":"dfd6137d-b7d4-45bd-8c87-b1b8263e7cd1","executionInfo":{"status":"ok","timestamp":1707812280438,"user_tz":-330,"elapsed":19,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p ./Testing\\ Videos/preproc_real_vids\n!mkdir -p ./Testing\\ Videos/preproc_fake_vids","metadata":{"id":"ac1fbd78-1027-4d21-935e-de0c1bf59952","executionInfo":{"status":"ok","timestamp":1707812281121,"user_tz":-330,"elapsed":700,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir ./Remaining\\ Real\\ Videos","metadata":{"id":"2746e59a-57d6-46f6-b517-769a0ea4abf8","executionInfo":{"status":"ok","timestamp":1707812281122,"user_tz":-330,"elapsed":9,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_fake_vids_paths = [\"./DeepfakeTIMIT/higher_quality\",\"./VidTIMIT\"]","metadata":{"id":"866e8f4c-d664-4dda-9235-980494532a22","executionInfo":{"status":"ok","timestamp":1707812281122,"user_tz":-330,"elapsed":8,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analyzing Training data\nMaking fake and real videos equal in quantity.","metadata":{"id":"TysNg7j0pgti"}},{"cell_type":"code","source":"for base_vid_path in real_fake_vids_paths:\n\n    vid_counter = 0\n    for single_vid_path in pathlib.Path(base_vid_path).glob(\"*/*.avi\"):\n\n        vid_counter += 1\n\n    print(\"Total Number of Videos are {}\".format(vid_counter))\n    vid_counter = 0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1707812281122,"user":{"displayName":"Noel William","userId":"06404726489186135363"},"user_tz":-330},"id":"355f0f44-ab11-4c87-b00a-a9a12c7ea273","outputId":"b39abe17-b8e9-4f27-8d84-e6535f0ca7a1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vid_counter = 0\n\nfor single_vid_path in pathlib.Path(real_fake_vids_paths[1]).glob(\"*/*.avi\"):\n\n    if str(single_vid_path.parts[-2]) in os.listdir(real_fake_vids_paths[0]):\n        vid_counter += 1\n\n    else:\n        if not os.path.isdir(\"./Remaining Real Videos/\"+single_vid_path.parts[-2]):\n            os.mkdir(\"./Remaining Real Videos/\"+single_vid_path.parts[-2])\n\n        shutil.move(src=single_vid_path,\n                    dst=\"./Remaining Real Videos/\"+\"/\".join([single_vid_path.parts[-2],\n                                                           single_vid_path.parts[-1]]))\n\nprint(\"Total Number of Videos are {}\".format(vid_counter))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":485,"status":"ok","timestamp":1707812325883,"user":{"displayName":"Noel William","userId":"06404726489186135363"},"user_tz":-330},"id":"a1d55d25-69fb-4f0e-8b0f-d498c6ae8cb7","outputId":"eda22de7-5962-47ba-a8e9-67a56a62acef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for base_vid_path in real_fake_vids_paths:\n\n    vid_counter = 0\n    for single_vid_path in pathlib.Path(base_vid_path).glob(\"*/*.avi\"):\n\n        vid_counter += 1\n\n    print(\"Total Number of Videos are {}\".format(vid_counter))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707812326356,"user":{"displayName":"Noel William","userId":"06404726489186135363"},"user_tz":-330},"id":"905103c7-c96d-440d-9c33-2a7c718cd507","outputId":"f85d430b-d33d-435e-dc21-b574c42325a9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions to preprocess training videos","metadata":{"id":"Xa81P9dspv12"}},{"cell_type":"code","source":"def detect_face(frame):\n\n    face_detector = MTCNN()\n    face_attr = face_detector.detect_faces(frame)\n\n    if len(face_attr) > 0:\n        if len(face_attr[0]['box']) > 0:\n            return face_attr[0]['box'], face_attr[0]['keypoints']['left_eye'], face_attr[0]['keypoints']['right_eye']\n    else:\n        return [None]*3","metadata":{"id":"9c3b1731-9510-43d4-b034-382d9d1b27b7","executionInfo":{"status":"ok","timestamp":1707812329366,"user_tz":-330,"elapsed":4,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def determine_rotation_angle(left_eye_kp, right_eye_kp):\n\n    delta_y = right_eye_kp[1] - left_eye_kp[1]\n    delta_x = right_eye_kp[0] - left_eye_kp[0]\n\n    return np.arctan(delta_y/delta_x)","metadata":{"id":"25894688-5a7f-4100-8efa-02e64dd58217","executionInfo":{"status":"ok","timestamp":1707812329367,"user_tz":-330,"elapsed":4,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def determine_scaling_factor(rescaled_img_size,left_eye_kp,right_eye_kp):\n\n    left_eye_kp = np.array(left_eye_kp)\n    right_eye_kp = np.array(right_eye_kp)\n\n    whole_frame_eye_dist = np.linalg.norm(x=(right_eye_kp - left_eye_kp),ord=2)\n\n    cropped_left_eye_kp = np.array([0.47*rescaled_img_size[0], 0.32*rescaled_img_size[1]])\n    cropped_right_eye_kp = np.array([0.47*rescaled_img_size[0], 0.68*rescaled_img_size[1]])\n\n    cropped_frame_eye_dist = np.linalg.norm(x=(cropped_right_eye_kp - cropped_left_eye_kp),\n                                            ord=2)\n\n    scaling_factor = cropped_frame_eye_dist/whole_frame_eye_dist\n\n    return scaling_factor, cropped_left_eye_kp, cropped_right_eye_kp","metadata":{"id":"eeb93385-0290-45e4-88b2-067336de51e6","executionInfo":{"status":"ok","timestamp":1707812329832,"user_tz":-330,"elapsed":3,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def determine_rotation_center(left_eye_kp, right_eye_kp):\n\n    center_x = (left_eye_kp[0] + right_eye_kp[0])/2\n    center_y = (left_eye_kp[1] + right_eye_kp[1])/2\n\n    return (center_x, center_y)","metadata":{"id":"6da5b795-4a18-414f-92c1-7e5b6cec2eed","executionInfo":{"status":"ok","timestamp":1707812330794,"user_tz":-330,"elapsed":2,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optional Step\nTesting Our functions","metadata":{"id":"P3CV7C7wnmmj"}},{"cell_type":"code","source":"'''\nsingle_vid_path = pathlib.Path(\"./DeepfakeTIMIT/higher_quality/mrgg0/si569-video-msjs1.avi\")\n\nprint(\"\\n\\nGoing to preprocess Video at {}\".format(single_vid_path))\n\nvideo = cv2.VideoCapture(str(single_vid_path))\ncropped_aligned_faces_list = list()\n\nwhile True:\n\n    is_frame, frame = video.read()\n\n    if not is_frame:\n        break\n\n    print(\"Going to preprocess Frame of {} having size {}\".format(single_vid_path.parts[-1],\n                                                                  frame.shape))\n\n    face_bbox, left_eye_kp, right_eye_kp = detect_face(frame)\n\n    if face_bbox == None:\n        continue\n\n    rotation_angle = determine_rotation_angle(left_eye_kp, right_eye_kp);\n    scaling_factor, cropped_left_eye_kp, cropped_right_eye_kp = determine_scaling_factor((224,224),\n                                                                                            left_eye_kp, right_eye_kp)\n    rotation_center = determine_rotation_center(left_eye_kp, right_eye_kp)\n\n    #print(\"Computed everything to get the Rotation Matrix for the Frame\")\n\n    rotation_matrix = cv2.getRotationMatrix2D(center=rotation_center,\n                                                         angle=rotation_angle,scale=scaling_factor)\n\n    #print(\"Computed the Rotation Matrix for the Frame\")\n\n    cropped_center_x = cropped_left_eye_kp[0]\n    cropped_center_y = cropped_left_eye_kp[1] + (np.linalg.norm(x=cropped_right_eye_kp-cropped_right_eye_kp,ord=2)/2)\n    cropped_center = (cropped_center_x, cropped_center_y)\n\n    rotation_matrix[0,2] = rotation_matrix[0,2] - (rotation_center[0] - cropped_center[0])\n    rotation_matrix[1,2] = rotation_matrix[1,2] - (rotation_center[1] - cropped_center[1])\n\n    #print(\"Translated the center in Rotation Matrix to the Cropped Frame Center\")\n\n    cropped_aligned_face = cv2.warpAffine(frame,rotation_matrix,(224,224),\n                                                      cv2.INTER_CUBIC)\n\n    #print(\"Cropped and Aligned the face in the Frame\")\n\n    cropped_aligned_faces_list.append(cropped_aligned_face)\n\n    print(\"Preprocessed Frame of {}\".format(single_vid_path.parts[-1]))\n\n    plt.imshow(cropped_aligned_face)\n\n    break;\n\nvideo.release()\n\ncropped_aligned_faces_list = np.array(cropped_aligned_faces_list)\n\n\nif \"DeepfakeTIMIT\" in list(single_vid_path.parts):\n    np.savez(\"./Training Videos/preproc_fake_vids/{}.npz\".format(str(single_vid_path.parts[-1]).replace(\".avi\",\"\")),\n                             args=cropped_aligned_faces_list)\nelse:\n    np.savez(\"./Training Videos/preproc_real_vids/{}.npz\".format(str(single_vid_path.parts[-1]).replace(\".avi\",\"\")),\n                             args=cropped_aligned_faces_list)\n\nprint(\"\\n\\nProcessed Video at {}\".format(single_vid_path))\n'''","metadata":{"id":"d3be37a7-333b-49ff-948a-e280f96bf29b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ndef preproc_single_vid(single_vid_path):\n\n    print(\"\\n\\nGoing to preprocess Video at {}\\n\".format(single_vid_path))\n\n    video = cv2.VideoCapture(str(single_vid_path))\n    cropped_aligned_faces_list = list()\n\n    while True:\n\n        is_frame, frame = video.read()\n\n        if not is_frame:\n            break\n\n        print(\"Going to preprocess frame of {}\".format(single_vid_path.parts[-1]))\n\n        face_bbox, left_eye_kp, right_eye_kp = detect_face(frame)\n        angle_bw_eyes = determine_rotation_angle(left_eye_kp, right_eye_kp);\n        scaling_factor, cropped_left_eye_kp, cropped_right_eye_kp = determine_scaling_factor((224,224),\n                                                                                        left_eye_kp, right_eye_kp)\n        rotation_center = determine_rotation_center(left_eye_kp, right_eye_kp)\n\n        rotation_matrix = cv2.getRotationMatrix2D(center=rotation_center,\n                                                     angle=rotation_angle,scale=scaling_factor)\n\n        cropped_center_x = cropped_left_eye_kp[0]\n        cropped_center_y = cropped_left_eye_kp[1] + (np.linalg.norm(x=cropped_right_eye_kp-cropped_right_eye_kp,ord=2)/2)\n        cropped_center = (cropped_center_x, cropped_center_y)\n\n        rotation_matrix[0,2] = rotation_matrix[0,2] - (rotation_center[0] - cropped_center[0])\n        rotation_matrix[1,2] = rotation_matrix[1,2] - (rotation_center[1] - cropped_center[1])\n\n        cropped_aligned_face = cv2.warpAffine(frame,rotation_matrix,(224,224),\n                                                  cv2.INTER_CUBIC)\n\n        cropped_aligned_faces_list.append(cropped_aligned_face)\n\n    video.release()\n\n    cropped_aligned_faces_list = np.array(cropped_aligned_faces_list)\n\n    if \"DeepfakeTIMIT\" in list(single_vid_path.parts):\n        np.savez(\"./Training Videos/preproc_fake_vids/{}.npz\".format(single_vid_path.parts[-1]),\n                         args=cropped_aligned_faces_list)\n    else:\n        np.savez(\"./Training Videos/preproc_real_vids/{}.npz\".format(single_vid_path.parts[-1]),\n                         args=cropped_aligned_faces_list)\n\n    print(\"\\n\\nProcessed Video at {}\".format(single_vid_path))\n\"\"\"","metadata":{"id":"b87504ba-6a71-43fe-89b9-d95d6dc06c0b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Function to preprocess a single video\nThis function uses all the function made previously.","metadata":{"id":"pRR9VF0bnzqZ"}},{"cell_type":"code","source":"def preproc_single_vid(mb_vids_paths):\n\n    for single_vid_path in mb_vids_paths:\n\n        print(\"\\n\\nGoing to preprocess Video at {}\".format(single_vid_path))\n\n        video = cv2.VideoCapture(str(single_vid_path))\n        cropped_aligned_faces_list = list()\n\n        while True:\n\n            is_frame, frame = video.read()\n\n            if not is_frame:\n                break\n            \"\"\"\n            print(\"Going to preprocess Frame of {} having size {}\".format(single_vid_path.parts[-1],\n                                                                  frame.shape))\n            \"\"\"\n            face_bbox, left_eye_kp, right_eye_kp = detect_face(frame)\n\n            if face_bbox == None:\n                continue\n\n            rotation_angle = determine_rotation_angle(left_eye_kp, right_eye_kp);\n            scaling_factor, cropped_left_eye_kp, cropped_right_eye_kp = determine_scaling_factor((224,224),\n                                                                                            left_eye_kp, right_eye_kp)\n            rotation_center = determine_rotation_center(left_eye_kp, right_eye_kp)\n\n            #print(\"Computed everything to get the Rotation Matrix for the Frame\")\n\n            rotation_matrix = cv2.getRotationMatrix2D(center=rotation_center,\n                                                         angle=rotation_angle,scale=scaling_factor)\n\n            #print(\"Computed the Rotation Matrix for the Frame\")\n\n            cropped_center_x = cropped_left_eye_kp[0]\n            cropped_center_y = cropped_left_eye_kp[1] + (np.linalg.norm(x=cropped_right_eye_kp-cropped_right_eye_kp,ord=2)/2)\n            cropped_center = (cropped_center_x, cropped_center_y)\n\n            rotation_matrix[0,2] = rotation_matrix[0,2] - (rotation_center[0] - cropped_center[0])\n            rotation_matrix[1,2] = rotation_matrix[1,2] - (rotation_center[1] - cropped_center[1])\n\n            #print(\"Translated the center in Rotation Matrix to the Cropped Frame Center\")\n\n            cropped_aligned_face = cv2.warpAffine(frame,rotation_matrix,(224,224),\n                                                      cv2.INTER_CUBIC)\n\n            #print(\"Cropped and Aligned the face in the Frame\")\n\n            cropped_aligned_faces_list.append(cropped_aligned_face)\n\n            print(\"Preprocessed Frame of {}\".format(single_vid_path.parts[-1]))\n\n        video.release()\n\n        cropped_aligned_faces_list = np.array(cropped_aligned_faces_list)\n\n        if \"DeepfakeTIMIT\" in list(single_vid_path.parts):\n            np.savez(\"./Training Videos/preproc_fake_vids/{}.npz\".format(str(single_vid_path.parts[-1]).replace(\".avi\",\"\")),\n                             args=cropped_aligned_faces_list)\n        else:\n            np.savez(\"./Training Videos/preproc_real_vids/{}.npz\".format(str(single_vid_path.parts[-1]).replace(\".avi\",\"\")),\n                             args=cropped_aligned_faces_list)\n\n        print(\"\\n\\nProcessed Video at {}\".format(single_vid_path))","metadata":{"id":"9ab3b951-5498-4e7e-a3f6-93ab42bcfd70","executionInfo":{"status":"ok","timestamp":1707812339706,"user_tz":-330,"elapsed":563,"user":{"displayName":"Noel William","userId":"06404726489186135363"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing all videos","metadata":{"id":"TemjW9VzoAwE"}},{"cell_type":"code","source":"from concurrent.futures import ThreadPoolExecutor\nfrom collections import deque\n\nvid_counter = 0\nmb_number = 0\n\nreal_fake_preproc_vids_paths = [\"./Training Videos/preproc_fake_vids\",\n                               \"./Training Videos/preproc_real_vids\"]\n\nfor vids_path, preproc_vids_path in zip(real_fake_vids_paths,\n                                                    real_fake_preproc_vids_paths):\n\n    vids2preproc_path_list = deque()\n\n    for single_vid_path in pathlib.Path(vids_path).glob(\"*/*.avi\"):\n\n        vids2preproc_path_list.append(single_vid_path)\n        vid_counter += 1\n\n        if vid_counter % (os.cpu_count()) == 0:\n\n            start_time = time.time()\n\n            with ThreadPoolExecutor(max_workers=(os.cpu_count())) as pool:\n                future = pool.submit(preproc_single_vid, list(vids2preproc_path_list))\n                future.result()\n\n            end_time = time.time()\n            mb_number += 1\n            elapsed_time = end_time - start_time\n\n            print(\"\\n\\nProcessed Mini Batch # {} of 64 Videos in {} seconds\".format(mb_number,elapsed_time))\n\n            vids2preproc_path_list.clear()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0h_q1c4YqynY","outputId":"44da84a6-f505-4241-acc2-84a36c0d4ecc","scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Storing Training Videos\nSeperately storing training fake and real videos in seperate list or numpy arrays","metadata":{"id":"47N6UP6-lSTo"}},{"cell_type":"code","source":"training_real_vids_paths = list()\ntraining_fake_vids_paths = list()\n\nfor single_img_path in pathlib.Path(\"/kaggle/input/deep-fake-and-real-videos/Training Videos\").glob(\"*/*.npz\"):\n\n    if \"real\" in str(single_img_path.parts[-2]).split(\"_\"):\n        training_real_vids_paths.append(str(single_img_path))\n\n    elif \"fake\" in str(single_img_path.parts[-2]).split(\"_\"):\n        training_fake_vids_paths.append(str(single_img_path))\n\ntraining_real_vids_paths = np.array(training_real_vids_paths)\ntraining_fake_vids_paths = np.array(training_fake_vids_paths)","metadata":{"id":"ed592b1d-e922-4600-a488-043c7a17e9dc","execution":{"iopub.status.busy":"2024-02-13T21:46:12.521612Z","iopub.execute_input":"2024-02-13T21:46:12.522004Z","iopub.status.idle":"2024-02-13T21:46:12.664716Z","shell.execute_reply.started":"2024-02-13T21:46:12.521977Z","shell.execute_reply":"2024-02-13T21:46:12.663872Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"len(training_real_vids_paths)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7yBsj4yQxLH","executionInfo":{"status":"ok","timestamp":1707757216235,"user_tz":-330,"elapsed":4,"user":{"displayName":"Noel William","userId":"06404726489186135363"}},"outputId":"621b4324-04cf-46f9-b6ce-4590e24467fc","execution":{"iopub.status.busy":"2024-02-13T21:46:12.665835Z","iopub.execute_input":"2024-02-13T21:46:12.666133Z","iopub.status.idle":"2024-02-13T21:46:12.67361Z","shell.execute_reply.started":"2024-02-13T21:46:12.666108Z","shell.execute_reply":"2024-02-13T21:46:12.67267Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"197"},"metadata":{}}]},{"cell_type":"code","source":"len(training_fake_vids_paths)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aSKWUuExQyr2","executionInfo":{"status":"ok","timestamp":1707757241901,"user_tz":-330,"elapsed":756,"user":{"displayName":"Noel William","userId":"06404726489186135363"}},"outputId":"1190534a-206e-4438-90a6-02ee798fbce7","execution":{"iopub.status.busy":"2024-02-13T21:46:12.674828Z","iopub.execute_input":"2024-02-13T21:46:12.675126Z","iopub.status.idle":"2024-02-13T21:46:12.684011Z","shell.execute_reply.started":"2024-02-13T21:46:12.675103Z","shell.execute_reply":"2024-02-13T21:46:12.683068Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"320"},"metadata":{}}]},{"cell_type":"code","source":"# Calculate how many times to repeat the real videos\nrepeat_times = len(training_fake_vids_paths) // len(training_real_vids_paths)\n\n# Repeat the real videos\noversampled_real_vids_paths = np.repeat(training_real_vids_paths, repeat_times)\n\n# If there are still fewer real videos than fake videos, append some real videos to make up the difference\nremainder = len(training_fake_vids_paths) - len(oversampled_real_vids_paths)\nif remainder > 0:\n    oversampled_real_vids_paths = np.concatenate((oversampled_real_vids_paths, training_real_vids_paths[:remainder]))\n\n# Now oversampled_real_vids_paths should have the same length as training_fake_vids_paths\nassert len(oversampled_real_vids_paths) == len(training_fake_vids_paths)","metadata":{"id":"696pixNASaye","execution":{"iopub.status.busy":"2024-02-13T21:46:12.68553Z","iopub.execute_input":"2024-02-13T21:46:12.685932Z","iopub.status.idle":"2024-02-13T21:46:12.693256Z","shell.execute_reply.started":"2024-02-13T21:46:12.6859Z","shell.execute_reply":"2024-02-13T21:46:12.692433Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"training_real_vids_paths = oversampled_real_vids_paths\nlen(training_real_vids_paths)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KTBYvXIcSf7m","executionInfo":{"status":"ok","timestamp":1707757775177,"user_tz":-330,"elapsed":4,"user":{"displayName":"Noel William","userId":"06404726489186135363"}},"outputId":"783edede-336a-4995-a731-0e3f8c93d7ef","execution":{"iopub.status.busy":"2024-02-13T21:46:12.694674Z","iopub.execute_input":"2024-02-13T21:46:12.694959Z","iopub.status.idle":"2024-02-13T21:46:12.705031Z","shell.execute_reply.started":"2024-02-13T21:46:12.694936Z","shell.execute_reply":"2024-02-13T21:46:12.704088Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"320"},"metadata":{}}]},{"cell_type":"markdown","source":"#Training Data Generator","metadata":{"id":"4GvSXC0omClt"}},{"cell_type":"code","source":"def custom_training_data_generator(mb_size, target_frame_size=(224, 224), max_frames=100):\n    for i in range(len(training_fake_vids_paths) // 2):\n        X_train_mb = []\n        Y_train_mb = []\n\n        training_fake_mb_vids_paths = training_fake_vids_paths[i * (mb_size // 2):(i + 1) * (mb_size // 2)]\n        training_real_mb_vids_paths = training_real_vids_paths[i * (mb_size // 2):(i + 1) * (mb_size // 2)]\n\n        for single_fake_vid_path, single_real_vid_path in zip(training_fake_mb_vids_paths,\n                                                              training_real_mb_vids_paths):\n\n            fake_frames = np.load(file=single_fake_vid_path)['args']\n            # Setting image to (224, 224) if not already\n            fake_frames_resized = [cv2.resize(frame, target_frame_size) for frame in fake_frames]\n            fake_frames_resized += [np.zeros_like(fake_frames_resized[0])] * (max_frames - len(fake_frames_resized))\n            fake_frames_resized = fake_frames_resized[:max_frames]  # This ensures 100 frames\n\n\n            real_frames = np.load(file=single_real_vid_path)['args']\n\n            real_frames_resized = [cv2.resize(frame, target_frame_size) for frame in real_frames]\n            real_frames_resized += [np.zeros_like(real_frames_resized[0])] * (max_frames - len(real_frames_resized))\n            real_frames_resized = real_frames_resized[:max_frames]\n\n            X_train_mb.append(real_frames_resized)\n            X_train_mb.append(fake_frames_resized)\n\n            # Real labeled as 0 and Fake labeled as 1\n            Y_train_mb.append(0)\n            Y_train_mb.append(1)\n\n        X_train_mb = np.array(X_train_mb)\n        Y_train_mb = np.array(Y_train_mb)\n\n        yield X_train_mb, Y_train_mb","metadata":{"id":"67kD8wBsvbF2","execution":{"iopub.status.busy":"2024-02-13T21:46:12.709419Z","iopub.execute_input":"2024-02-13T21:46:12.710374Z","iopub.status.idle":"2024-02-13T21:46:12.720926Z","shell.execute_reply.started":"2024-02-13T21:46:12.710337Z","shell.execute_reply":"2024-02-13T21:46:12.719971Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def custom_training_data_generator(mb_size, target_frame_size=(224, 224), max_frames=100):\n    while True:  # Add this line to loop indefinitely over your data\n        for i in range(len(training_fake_vids_paths) // 2):\n            X_train_mb = []\n            Y_train_mb = []\n\n            training_fake_mb_vids_paths = training_fake_vids_paths[i * (mb_size // 2):(i + 1) * (mb_size // 2)]\n            training_real_mb_vids_paths = training_real_vids_paths[i * (mb_size // 2):(i + 1) * (mb_size // 2)]\n\n            for single_fake_vid_path, single_real_vid_path in zip(training_fake_mb_vids_paths,\n                                                                  training_real_mb_vids_paths):\n\n                fake_frames = np.load(file=single_fake_vid_path)['args']\n                # Setting image to (224, 224) if not already\n                fake_frames_resized = [cv2.resize(frame, target_frame_size) for frame in fake_frames]\n                fake_frames_resized += [np.zeros_like(fake_frames_resized[0])] * (max_frames - len(fake_frames_resized))\n                fake_frames_resized = fake_frames_resized[:max_frames]  # This ensures 100 frames\n\n\n                real_frames = np.load(file=single_real_vid_path)['args']\n\n                real_frames_resized = [cv2.resize(frame, target_frame_size) for frame in real_frames]\n                real_frames_resized += [np.zeros_like(real_frames_resized[0])] * (max_frames - len(real_frames_resized))\n                real_frames_resized = real_frames_resized[:max_frames]\n\n                X_train_mb.append(real_frames_resized)\n                X_train_mb.append(fake_frames_resized)\n\n                # Real labeled as 0 and Fake labeled as 1\n                Y_train_mb.append(0)\n                Y_train_mb.append(1)\n\n            X_train_mb = np.array(X_train_mb)\n            Y_train_mb = np.array(Y_train_mb)\n\n            yield X_train_mb, Y_train_mb","metadata":{"id":"PUGiJg90V_gs","execution":{"iopub.status.busy":"2024-02-13T21:46:12.722356Z","iopub.execute_input":"2024-02-13T21:46:12.722694Z","iopub.status.idle":"2024-02-13T21:46:12.734074Z","shell.execute_reply.started":"2024-02-13T21:46:12.722644Z","shell.execute_reply":"2024-02-13T21:46:12.733098Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Split training data for validation data\nCreating seperate data for validation same as training data","metadata":{"id":"JfmMXW_-mMVu"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Combine real and fake paths\nall_vids_paths = np.concatenate((training_real_vids_paths, training_fake_vids_paths))\n\n# Create labels: 0 for real and 1 for fake\nall_vids_labels = np.concatenate((np.zeros(len(training_real_vids_paths)), np.ones(len(training_fake_vids_paths))))\n\n# Split the data into training and validation sets (80-20 split here)\ntrain_vids_paths, val_vids_paths, train_vids_labels, val_vids_labels = train_test_split(all_vids_paths, all_vids_labels, test_size=0.2, random_state=42)\n\n# Separate real and fake validation video paths\nvalidation_real_vids_paths = val_vids_paths[val_vids_labels == 0]\nvalidation_fake_vids_paths = val_vids_paths[val_vids_labels == 1]\n","metadata":{"id":"FDDyTvLfKWlC","execution":{"iopub.status.busy":"2024-02-13T21:46:12.735438Z","iopub.execute_input":"2024-02-13T21:46:12.735822Z","iopub.status.idle":"2024-02-13T21:46:13.114821Z","shell.execute_reply.started":"2024-02-13T21:46:12.735789Z","shell.execute_reply":"2024-02-13T21:46:13.114006Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"#Validation Data Generator","metadata":{"id":"UXuBpGfimbRQ"}},{"cell_type":"code","source":"def custom_validation_data_generator(mb_size, target_frame_size=(224, 224), max_frames=100):\n    for i in range(len(validation_fake_vids_paths) // 2):\n        X_val_mb = []\n        Y_val_mb = []\n\n        validation_fake_mb_vids_paths = validation_fake_vids_paths[i * (mb_size // 2):(i + 1) * (mb_size // 2)]\n        validation_real_mb_vids_paths = validation_real_vids_paths[i * (mb_size // 2):(i + 1) * (mb_size // 2)]\n\n        for single_fake_vid_path, single_real_vid_path in zip(validation_fake_mb_vids_paths,\n                                                              validation_real_mb_vids_paths):\n\n            fake_frames = np.load(file=single_fake_vid_path)['args']\n            # Setting image to (224, 224) if not already\n            fake_frames_resized = [cv2.resize(frame, target_frame_size) for frame in fake_frames]\n            fake_frames_resized += [np.zeros_like(fake_frames_resized[0])] * (max_frames - len(fake_frames_resized))\n            fake_frames_resized = fake_frames_resized[:max_frames]  # This ensures 100 frames\n\n\n            real_frames = np.load(file=single_real_vid_path)['args']\n\n            real_frames_resized = [cv2.resize(frame, target_frame_size) for frame in real_frames]\n            real_frames_resized += [np.zeros_like(real_frames_resized[0])] * (max_frames - len(real_frames_resized))\n            real_frames_resized = real_frames_resized[:max_frames]\n\n            X_val_mb.append(real_frames_resized)\n            X_val_mb.append(fake_frames_resized)\n\n            # Real labeled as 0 and Fake labeled as 1\n            Y_val_mb.append(0)\n            Y_val_mb.append(1)\n\n        X_val_mb = np.array(X_val_mb)\n        Y_val_mb = np.array(Y_val_mb)\n\n        yield X_val_mb, Y_val_mb","metadata":{"id":"VS9q3TIYKTxD","execution":{"iopub.status.busy":"2024-02-13T21:46:13.11601Z","iopub.execute_input":"2024-02-13T21:46:13.116322Z","iopub.status.idle":"2024-02-13T21:46:13.127412Z","shell.execute_reply.started":"2024-02-13T21:46:13.116288Z","shell.execute_reply":"2024-02-13T21:46:13.126466Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def custom_validation_data_generator(mb_size, target_frame_size=(224, 224), max_frames=100):\n    while True:  # Add this line to loop indefinitely over your data\n        for i in range(len(validation_fake_vids_paths) // 2):\n            X_val_mb = []\n            Y_val_mb = []\n\n            validation_fake_mb_vids_paths = validation_fake_vids_paths[i * (mb_size // 2):(i + 1) * (mb_size // 2)]\n            validation_real_mb_vids_paths = validation_real_vids_paths[i * (mb_size // 2):(i + 1) * (mb_size // 2)]\n\n            for single_fake_vid_path, single_real_vid_path in zip(validation_fake_mb_vids_paths,\n                                                                  validation_real_mb_vids_paths):\n\n                fake_frames = np.load(file=single_fake_vid_path)['args']\n                # Setting image to (224, 224) if not already\n                fake_frames_resized = [cv2.resize(frame, target_frame_size) for frame in fake_frames]\n                fake_frames_resized += [np.zeros_like(fake_frames_resized[0])] * (max_frames - len(fake_frames_resized))\n                fake_frames_resized = fake_frames_resized[:max_frames]  # This ensures 100 frames\n\n\n                real_frames = np.load(file=single_real_vid_path)['args']\n\n                real_frames_resized = [cv2.resize(frame, target_frame_size) for frame in real_frames]\n                real_frames_resized += [np.zeros_like(real_frames_resized[0])] * (max_frames - len(real_frames_resized))\n                real_frames_resized = real_frames_resized[:max_frames]\n\n                X_val_mb.append(real_frames_resized)\n                X_val_mb.append(fake_frames_resized)\n\n                # Real labeled as 0 and Fake labeled as 1\n                Y_val_mb.append(0)\n                Y_val_mb.append(1)\n\n            X_val_mb = np.array(X_val_mb)\n            Y_val_mb = np.array(Y_val_mb)\n\n            yield X_val_mb, Y_val_mb","metadata":{"id":"Nt7maX4EWCYk","execution":{"iopub.status.busy":"2024-02-13T21:46:13.128444Z","iopub.execute_input":"2024-02-13T21:46:13.128795Z","iopub.status.idle":"2024-02-13T21:46:13.140585Z","shell.execute_reply.started":"2024-02-13T21:46:13.128768Z","shell.execute_reply":"2024-02-13T21:46:13.139709Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"#Initializing data Generators","metadata":{"id":"mTYuosYSnGHy"}},{"cell_type":"markdown","source":"Initializing our data generator and checking shape of data for our model","metadata":{"id":"cpUqq-sLmir0"}},{"cell_type":"code","source":"train_datagen = custom_training_data_generator(4)","metadata":{"id":"47184945-cc23-45ad-b452-89152cc83ce7","execution":{"iopub.status.busy":"2024-02-13T21:46:13.141625Z","iopub.execute_input":"2024-02-13T21:46:13.142018Z","iopub.status.idle":"2024-02-13T21:46:13.152191Z","shell.execute_reply.started":"2024-02-13T21:46:13.141985Z","shell.execute_reply":"2024-02-13T21:46:13.15124Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"val_datagen = custom_validation_data_generator(4)","metadata":{"id":"284QmVfaLi6W","execution":{"iopub.status.busy":"2024-02-13T21:46:13.153193Z","iopub.execute_input":"2024-02-13T21:46:13.154409Z","iopub.status.idle":"2024-02-13T21:46:13.160204Z","shell.execute_reply.started":"2024-02-13T21:46:13.154379Z","shell.execute_reply":"2024-02-13T21:46:13.15932Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"X_train_mb, Y_train_mb = next(train_datagen)","metadata":{"id":"58ff5239-15b3-4a37-8f64-f966b222a59e","execution":{"iopub.status.busy":"2024-02-13T21:46:13.161285Z","iopub.execute_input":"2024-02-13T21:46:13.161548Z","iopub.status.idle":"2024-02-13T21:46:13.7413Z","shell.execute_reply.started":"2024-02-13T21:46:13.161525Z","shell.execute_reply":"2024-02-13T21:46:13.740344Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"X_train_mb.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1707758755930,"user":{"displayName":"Noel William","userId":"06404726489186135363"},"user_tz":-330},"id":"ca140a8a-ed6c-487a-9c2d-201408c1e512","outputId":"7564ac62-56f0-4c7b-cfe7-d6910a76462e","execution":{"iopub.status.busy":"2024-02-13T21:46:13.74249Z","iopub.execute_input":"2024-02-13T21:46:13.7428Z","iopub.status.idle":"2024-02-13T21:46:13.74892Z","shell.execute_reply.started":"2024-02-13T21:46:13.742775Z","shell.execute_reply":"2024-02-13T21:46:13.747899Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(4, 100, 224, 224, 3)"},"metadata":{}}]},{"cell_type":"code","source":"Y_train_mb.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1130,"status":"ok","timestamp":1707758757971,"user":{"displayName":"Noel William","userId":"06404726489186135363"},"user_tz":-330},"id":"qMO6dF0Vb3il","outputId":"55b28f54-5a96-4b05-d368-61bfe65cfb54","execution":{"iopub.status.busy":"2024-02-13T21:46:13.750175Z","iopub.execute_input":"2024-02-13T21:46:13.75047Z","iopub.status.idle":"2024-02-13T21:46:13.760246Z","shell.execute_reply.started":"2024-02-13T21:46:13.750446Z","shell.execute_reply":"2024-02-13T21:46:13.759298Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(4,)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Neural Network","metadata":{"id":"QMfPNXZYmtx_"}},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import TimeDistributed, LSTM, Dense, Flatten, Dropout, Input\nfrom tensorflow.keras.applications import ResNet50","metadata":{"id":"Z-ymocTZe-Bk","execution":{"iopub.status.busy":"2024-02-13T21:46:13.761484Z","iopub.execute_input":"2024-02-13T21:46:13.761854Z","iopub.status.idle":"2024-02-13T21:46:13.767937Z","shell.execute_reply.started":"2024-02-13T21:46:13.761827Z","shell.execute_reply":"2024-02-13T21:46:13.767066Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def create_model():\n\n    base_model = ResNet50(include_top=False, input_shape=(224, 224, 3))\n    base_model.trainable = False\n\n    inputs = Input(shape=(100, 224, 224, 3))\n\n    x = TimeDistributed(base_model)(inputs)\n\n    x = TimeDistributed(Flatten())(x)\n\n    x = LSTM(50, return_sequences=False)(x)\n\n    x = Dropout(0.25)(x)\n\n    outputs = Dense(1, activation=\"sigmoid\")(x)\n\n    model = Model(inputs=inputs, outputs=outputs)\n\n    return model","metadata":{"id":"e8573d32-ef0e-4c36-833c-eb1741366147","execution":{"iopub.status.busy":"2024-02-13T21:46:13.769049Z","iopub.execute_input":"2024-02-13T21:46:13.76935Z","iopub.status.idle":"2024-02-13T21:46:13.776843Z","shell.execute_reply.started":"2024-02-13T21:46:13.769325Z","shell.execute_reply":"2024-02-13T21:46:13.775937Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model = create_model()","metadata":{"id":"nXIZTsT6ez3_","execution":{"iopub.status.busy":"2024-02-13T21:46:13.778396Z","iopub.execute_input":"2024-02-13T21:46:13.778889Z","iopub.status.idle":"2024-02-13T21:46:18.836584Z","shell.execute_reply.started":"2024-02-13T21:46:13.778857Z","shell.execute_reply":"2024-02-13T21:46:18.835728Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n94765736/94765736 [==============================] - 1s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"id":"8utQvokZe0p1","execution":{"iopub.status.busy":"2024-02-13T21:46:18.8392Z","iopub.execute_input":"2024-02-13T21:46:18.839519Z","iopub.status.idle":"2024-02-13T21:46:18.861713Z","shell.execute_reply.started":"2024-02-13T21:46:18.83949Z","shell.execute_reply":"2024-02-13T21:46:18.860713Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1707758764574,"user":{"displayName":"Noel William","userId":"06404726489186135363"},"user_tz":-330},"id":"AZVsi50ogfAK","outputId":"2646a788-d616-4728-f2de-5a79ae58d358","execution":{"iopub.status.busy":"2024-02-13T21:46:18.863065Z","iopub.execute_input":"2024-02-13T21:46:18.863901Z","iopub.status.idle":"2024-02-13T21:46:18.902984Z","shell.execute_reply.started":"2024-02-13T21:46:18.863873Z","shell.execute_reply":"2024-02-13T21:46:18.902048Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 100, 224, 224, 3  0         \n                             )]                                  \n                                                                 \n time_distributed (TimeDistr  (None, 100, 7, 7, 2048)  23587712  \n ibuted)                                                         \n                                                                 \n time_distributed_1 (TimeDis  (None, 100, 100352)      0         \n tributed)                                                       \n                                                                 \n lstm (LSTM)                 (None, 50)                20080600  \n                                                                 \n dropout (Dropout)           (None, 50)                0         \n                                                                 \n dense (Dense)               (None, 1)                 51        \n                                                                 \n=================================================================\nTotal params: 43,668,363\nTrainable params: 20,080,651\nNon-trainable params: 23,587,712\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#Fitting Our Model","metadata":{"id":"JNXTYxJQm8hg"}},{"cell_type":"code","source":"train_steps_per_epoch = len(train_vids_paths) // 4  # Since your mini-batch size is 4\nval_steps = len(val_vids_paths) // 4","metadata":{"id":"-C4eg65WMuT7","execution":{"iopub.status.busy":"2024-02-13T21:46:18.90722Z","iopub.execute_input":"2024-02-13T21:46:18.907541Z","iopub.status.idle":"2024-02-13T21:46:18.91211Z","shell.execute_reply.started":"2024-02-13T21:46:18.907518Z","shell.execute_reply":"2024-02-13T21:46:18.910825Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"model.fit(train_datagen, steps_per_epoch=train_steps_per_epoch,\n                    validation_data=val_datagen, validation_steps=val_steps,\n                    epochs=10)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":210},"id":"QJn90vQdMwTc","executionInfo":{"status":"error","timestamp":1707760273449,"user_tz":-330,"elapsed":558,"user":{"displayName":"Noel William","userId":"06404726489186135363"}},"outputId":"ef4b0554-ac9d-457e-a3f4-79b5d7710317","execution":{"iopub.status.busy":"2024-02-13T21:46:18.913434Z","iopub.execute_input":"2024-02-13T21:46:18.914139Z","iopub.status.idle":"2024-02-13T22:20:40.003244Z","shell.execute_reply.started":"2024-02-13T21:46:18.914108Z","shell.execute_reply":"2024-02-13T22:20:40.002437Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Epoch 1/10\n128/128 [==============================] - 221s 2s/step - loss: 1.1485 - accuracy: 0.4980 - val_loss: 0.7819 - val_accuracy: 0.5000\nEpoch 2/10\n128/128 [==============================] - 204s 2s/step - loss: 0.7364 - accuracy: 0.5273 - val_loss: 0.6944 - val_accuracy: 0.5000\nEpoch 3/10\n128/128 [==============================] - 204s 2s/step - loss: 0.7141 - accuracy: 0.5000 - val_loss: 0.6938 - val_accuracy: 0.4922\nEpoch 4/10\n128/128 [==============================] - 205s 2s/step - loss: 0.7114 - accuracy: 0.4668 - val_loss: 0.6929 - val_accuracy: 0.5000\nEpoch 5/10\n128/128 [==============================] - 204s 2s/step - loss: 0.7138 - accuracy: 0.4863 - val_loss: 0.6931 - val_accuracy: 0.5000\nEpoch 6/10\n128/128 [==============================] - 204s 2s/step - loss: 0.7039 - accuracy: 0.5176 - val_loss: 0.6931 - val_accuracy: 0.4844\nEpoch 7/10\n128/128 [==============================] - 205s 2s/step - loss: 0.7117 - accuracy: 0.4668 - val_loss: 0.6932 - val_accuracy: 0.4922\nEpoch 8/10\n128/128 [==============================] - 205s 2s/step - loss: 0.7158 - accuracy: 0.4766 - val_loss: 0.6936 - val_accuracy: 0.4922\nEpoch 9/10\n128/128 [==============================] - 204s 2s/step - loss: 0.7056 - accuracy: 0.4941 - val_loss: 0.6928 - val_accuracy: 0.5156\nEpoch 10/10\n128/128 [==============================] - 204s 2s/step - loss: 0.7118 - accuracy: 0.4668 - val_loss: 0.6931 - val_accuracy: 0.5000\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7dce40238dc0>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"JkEw_frSQf7u"},"execution_count":null,"outputs":[]}]}